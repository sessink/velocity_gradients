{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/estimate_vorticity_from_data/')\n",
    "\n",
    "from scipy.signal import butter, lfilter, freqz,filtfilt\n",
    "from scipy import signal\n",
    "from haversine import haversine\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "import gsw\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.3,context='poster',style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    '''create butterworth filter for drifter trajectories'''\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    '''run butterworth filter forward and backward to remove edge-effects'''\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def datenum2datetime(matlab_datenum):\n",
    "    \"\"\"Convert Matlab datenum to python datetime object\n",
    "        at this point, only size-1 skalar can be converted.\n",
    "    \"\"\"\n",
    "    return datetime.fromordinal(int(matlab_datenum)) + timedelta(days=matlab_datenum%1) - timedelta(days = 366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddata = sio.loadmat('./asiri_RR1513_data.mat',squeeze_me=True,struct_as_record=False)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon=[];lat=[];ts=[];ids=[];uv=[];sal=[];sst=[]\n",
    "for i,data in enumerate(ddata):\n",
    "    lon.append(data.lon)\n",
    "    lat.append(data.lat)\n",
    "    sal.append(data.salinity)\n",
    "    sst.append(data.sst)\n",
    "    ts.append(data.ts)\n",
    "    uv.append(data.uv)\n",
    "    ids.append(np.ones_like(data.ts)*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=pd.DataFrame()\n",
    "for i,data in enumerate(ddata):\n",
    "    temp=pd.DataFrame({'sst':sst[i],'sal':sal[i],'lat':lat[i],'lon':lon[i],'uv':uv[i],'particle':i,'time':ts[i]})\n",
    "    temp['time']=temp['time'].apply(datenum2datetime)\n",
    "    temp.set_index('time',inplace=True)\n",
    "    dat = pd.concat( [dat,temp] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat.drop(dat[ ((dat.particle == 7) & (dat.index > '2015-11-20')) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.6/site-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n"
     ]
    }
   ],
   "source": [
    "p1 = dat[dat['particle']==1]\n",
    "ff=gsw.f( np.nanmin( p1['lat'] ) )\n",
    "Tinertial=  2*np.pi/ff # seconds\n",
    "order = 6\n",
    "#cutoff = 1/(1.2*Tinertial)  # desired cutoff frequency of the filter, Hz\n",
    "cutoff = 1/(3*60*60)  # desired cutoff frequency of the filter, Hz\n",
    "# 3h in seconds\n",
    "fs = 1/(3600)\n",
    "\n",
    "new=pd.DataFrame()\n",
    "for i in range(45):\n",
    "    p = dat[dat['particle']==i]\n",
    "    p.drop(p[~np.isfinite(p.lat)].index,inplace=True)\n",
    "    p.drop(p[~np.isfinite(p.lon)].index,inplace=True)\n",
    "    #p.sort_index(ascending=False,inplace=True)\n",
    "    temp = pd.DataFrame(index=p.index)\n",
    "    #temp['uv_filt'] = butter_lowpass_filter(p.uv, cutoff, fs, order)\n",
    "    temp['lat_filt'] = butter_lowpass_filter(p.lat, cutoff, fs, order)\n",
    "    temp['lon_filt'] = butter_lowpass_filter(p.lon, cutoff, fs, order)\n",
    "    temp['particle'] = i\n",
    "    temp['uv'] = p['uv']\n",
    "    temp['lon'] = p['lon']\n",
    "    temp['lat'] = p['lat']\n",
    "    temp['sst'] = p['sst']\n",
    "    temp['sal'] = p['sal']\n",
    "    new= pd.concat([new,temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon2uv_dir(lon,lat,direction):\n",
    "    ''' forward/backward differences\n",
    "    \n",
    "        convert time series of lon and lat into time series of u and v\n",
    "        using: haversine\n",
    "        \n",
    "        lat,lon: pandas columns,degrees\n",
    "        u,v: vectores, m/s\n",
    "    '''\n",
    "    dt = 60*60 #now in second\n",
    "    lon2 = lon.reindex(index=np.roll(lon.index,-1)).values\n",
    "    lat2 = lat.reindex(index=np.roll(lat.index,-1)).values\n",
    "    lon=lon.values\n",
    "    lat=lat.values\n",
    "    if direction=='back':\n",
    "        lon=np.flipud(lon)\n",
    "        lon2=np.flipud(lon2)\n",
    "        lat=np.flipud(lat)\n",
    "        lat2=np.flipud(lat2)\n",
    "        \n",
    "    dr = np.array( [haversine([lon[i],lat[i]],[lon2[i],lat2[i]]) for i in range(len(lon))] )\n",
    "\n",
    "    xx=np.sin(np.deg2rad(lon2-lon))*np.cos(np.deg2rad(lat2))\n",
    "    yy=np.cos(np.deg2rad(lat))*np.sin(np.deg2rad(lat2))-np.sin(np.deg2rad(lat))*np.cos(np.deg2rad(lat2))*np.cos(np.deg2rad(lon-lon))\n",
    "\n",
    "    gamma=np.arctan2(yy,xx)\n",
    "    \n",
    "    if direction=='back':\n",
    "        dr = np.flipud(dr)\n",
    "        gamma = np.flipud(gamma)\n",
    "    \n",
    "    c=1000;\n",
    "    u=c*dr/dt*np.cos(gamma)\n",
    "    v=c*dr/dt*np.sin(gamma)\n",
    "    return u,v\n",
    "\n",
    "def latlon2uv(lon,lat):\n",
    "    '''apply both forward and backward differences to get centered difference'''\n",
    "    u1,v1 = latlon2uv_dir(lon,lat,'for')\n",
    "    u2,v2 = latlon2uv_dir(lon,lat,'back')\n",
    "    \n",
    "    # correct the edges!\n",
    "    u1[-1]=u2[0]\n",
    "    v1[-1]=v2[0]\n",
    "    u2[-1]=u1[0]\n",
    "    v2[-1]=v1[0]\n",
    "    \n",
    "    uv = 0.5*(u1+u2) + 1j*0.5*(v1+v2)\n",
    "    return uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "total=pd.DataFrame()\n",
    "for i in range(45):\n",
    "    #print(i)\n",
    "    temp=new[new.particle==i]\n",
    "    lon=temp.lon_filt\n",
    "    lat=temp.lat_filt\n",
    "    uv=latlon2uv(lon,lat)\n",
    "    temp['uv_filt']=uv\n",
    "    total = pd.concat([total,temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_trajectories(df):\n",
    "    alldata=pd.DataFrame([]) # empty dataframe\n",
    "    for i in range(len(df.particle.unique())): \n",
    "        # loop over drifters\n",
    "        temp_df = df[df.particle==i] \n",
    "        tempp = temp_df.resample('1h').bfill(limit=1).interpolate('pchip') # if resampling has gaps, fill with last entry!\n",
    "        # reindexing not necessary\n",
    "        tempp.loc[:,'time']=tempp.index\n",
    "        tempp.set_index('particle',inplace=True)\n",
    "        tempp.loc[:,'particle']=tempp.index\n",
    "        # concat multiple drifters\n",
    "        alldata = pd.concat( [alldata,tempp] )\n",
    "    return alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = downsample_trajectories(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata['particle'] = np.round(alldata.particle).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.set_index('particle',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize storage data type if necessary (already adjusted in least square script)    \n",
    "df_float = alldata.select_dtypes(include=['float'])\n",
    "converted_float = df_float.apply(pd.to_numeric,downcast='float')\n",
    "optimized_df = alldata.copy()\n",
    "optimized_df[converted_float.columns] = converted_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print( len( optimized_df.index.unique() ) )\n",
    "optimized_df.drop(optimized_df[~np.isfinite(optimized_df.lat)].index,inplace=True)\n",
    "optimized_df.to_pickle('posveldata_3h.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 75744\n",
      "drwxr-xr-x@ 11 sebastian  staff   352B Dec 13 01:04 \u001b[34m.\u001b[m\u001b[m/\n",
      "drwxr-xr-x@  6 sebastian  staff   192B Sep  6 14:16 \u001b[34m..\u001b[m\u001b[m/\n",
      "drwxr-xr-x@  3 sebastian  staff    96B Dec 13 00:58 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 sebastian  staff    16M Oct 24  2017 asiri_RR1513_data.mat\n",
      "-rw-r--r--@  1 sebastian  staff   5.5M Dec  6  2016 data_hourly_120616.mat\n",
      "-rw-r--r--@  1 sebastian  staff   3.7M Oct 26  2017 posdata.pkl\n",
      "-rw-r--r--@  1 sebastian  staff   5.1M Nov  9  2017 posveldata.pkl\n",
      "-rw-r--r--@  1 sebastian  staff   7.6M Dec 13 01:04 posveldata_3h.pkl\n",
      "-rw-r--r--@  1 sebastian  staff   8.8M Mar 27  2018 posveldata_all.pkl\n",
      "-rw-r--r--@  1 sebastian  staff   7.2M Dec  2  2017 posveldata_filt.pkl\n",
      "-rw-r--r--@  1 sebastian  staff    14K Dec 13 01:04 read_filter_bin_pandas_data.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls -la -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sebastian/Dropbox (MIT)/jgr_deformation/data/drifter'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook read_filter_bin_pandas_data.ipynb to script\n",
      "[NbConvertApp] Writing 6401 bytes to read_filter_bin_pandas_data.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script *.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
